apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: raycluster-autoscaler
spec:
  # The version of Ray you are using. Make sure all Ray containers are running this version of Ray.
  rayVersion: '2.41.0'
  # If `enableInTreeAutoscaling` is true, the Autoscaler sidecar will be added to the Ray head pod.
  # Ray Autoscaler integration is Beta with KubeRay >= 0.3.0 and Ray >= 2.0.0.
  enableInTreeAutoscaling: true
  # `autoscalerOptions` is an OPTIONAL field specifying configuration overrides for the Ray Autoscaler.
  # The example configuration shown below represents the DEFAULT values.
  # (You may delete autoscalerOptions if the defaults are suitable.)
  autoscalerOptions:
    # `upscalingMode` is "Default" or "Aggressive."
    # Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.
    # Default: Upscaling is not rate-limited.
    # Aggressive: An alias for Default; upscaling is not rate-limited.
    upscalingMode: Default
    # `idleTimeoutSeconds` is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
    idleTimeoutSeconds: 60
    # `image` optionally overrides the Autoscaler's container image. The Autoscaler uses the same image as the Ray container by default.
    ## image: ray-ssh:latest
    # `imagePullPolicy` optionally overrides the Autoscaler container's default image pull policy (IfNotPresent).
    imagePullPolicy: IfNotPresent
    # Optionally specify the Autoscaler container's securityContext.
    securityContext: {}
    env: []
    envFrom: []
    # resources specifies optional resource request and limit overrides for the Autoscaler container.
    # The default Autoscaler resource limits and requests should be sufficient for production use-cases.
    # However, for large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.
    # resources:
    #   limits:
    #     cpu: "1000m"
    #     memory: "1024Mi"
    #   requests:
    #     cpu: "1000m"
    #     memory: "1024Mi"
  # Ray head pod template
  headGroupSpec:
    # The `rayStartParams` are used to configure the `ray start` command.
    # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
    # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
    rayStartParams:
      # Setting "num-cpus: 0" to avoid any Ray actors or tasks being scheduled on the Ray head Pod.
      num-cpus: "0"
      # Use `resources` to optionally specify custom resource annotations for the Ray node.
      # The value of `resources` is a string-integer mapping.
      # Currently, `resources` must be provided in the specific format demonstrated below:
      # resources: '"{\"Custom1\": 1, \"Custom2\": 5}"'
    # Pod template
    template:
      # metadata:
        # labels:
          # app: raycluster-autoscaler
      spec:
        containers:
        # The Ray head container
        - name: ray-head
          image: rayproject/ray:2.44.0.f468b3-py311-aarch64 
          # imagePullPolicy: Never
          imagePullPolicy: Always
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          - containerPort: 22
            name: ssh
          - containerPort: 44217
            name: as-metrics # autoscaler
          - containerPort: 44227
            name: dash-metrics # dashboard
          - containerPort: 9999
            name: code-server
          # lifecycle:
          #   postStart:
          #     exec:
          #       command: ["sudo", "/start.sh"]
          resources:
            limits:
              cpu: "2"
              memory: "3G"
            requests:
              cpu: "2"
              memory: "3G"
          volumeMounts:
            - mountPath: /home/ray/samples
              name: ray-example-configmap
            - mountPath: /tmp/ray
              name: ray-logs
            - mountPath: /home/ray/workspace  # Mount path inside container
              name: local-code
            # - name: ssh-authorized-keys
            #   mountPath: /home/ray/.ssh
          env:
            - name: RAY_GRAFANA_IFRAME_HOST
              value: http://127.0.0.1:3000
            - name: RAY_GRAFANA_HOST
              value: http://prometheus-grafana.prometheus-system.svc:80
            - name: RAY_PROMETHEUS_HOST
              value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090
        volumes:
          - name: ray-logs
            emptyDir: {}
          - name: local-code
            persistentVolumeClaim:
              claimName: local-code-pvc
          # - name: ssh-authorized-keys
          #   secret:
          #     secretName: ray-ssh-authorized-keys
          #     defaultMode: 0600
          - name: ray-example-configmap
            configMap:
              name: ray-example
              defaultMode: 0777
              items:
                - key: detached_actor.py
                  path: detached_actor.py
                - key: terminate_detached_actor.py
                  path: terminate_detached_actor.py
  workerGroupSpecs:
  # the Pod replicas in this group typed worker
  - replicas: 0
    minReplicas: 0
    maxReplicas: 10
    # logical group name, for this called small-group, also can be functional
    groupName: small-group
    # If worker pods need to be added, Ray Autoscaler can increment the `replicas`.
    # If worker pods need to be removed, Ray Autoscaler decrements the replicas, and populates the `workersToDelete` list.
    # KubeRay operator will remove Pods from the list until the desired number of replicas is satisfied.
    #scaleStrategy:
    #  workersToDelete:
    #  - raycluster-complete-worker-small-group-bdtwh
    #  - raycluster-complete-worker-small-group-hv457
    #  - raycluster-complete-worker-small-group-k8tj7
    rayStartParams: {}
    # Pod template
    template:
      spec:
        containers:
        - name: ray-worker
          # image: rayproject/ray:2.41.0
          image: rayproject/ray:2.44.0.f468b3-py311-aarch64
          imagePullPolicy: Always ## added to avoid pulling the image from the registry
          # imagePullPolicy: IfNotPresent ## added to avoid pulling the image from the registry
          volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
          resources:
            limits:
              cpu: "2"
              memory: "3G"
            requests:
              cpu: "2"
              memory: "3G"
        volumes:
          - name: ray-logs
            emptyDir: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-example
data:
  detached_actor.py: |
    import ray
    import sys

    @ray.remote(num_cpus=1)
    class Actor:
      pass

    ray.init(namespace="default_namespace")
    Actor.options(name=sys.argv[1], lifetime="detached").remote()

  terminate_detached_actor.py: |
    import ray
    import sys

    ray.init(namespace="default_namespace")
    detached_actor = ray.get_actor(sys.argv[1])
    ray.kill(detached_actor)
# ---
# apiVersion: v1
# kind: Service
# metadata:
#   name: ray-head-ssh
# spec:
#   selector:
#     ray.io/node-type: head
#     ray.io/cluster: raycluster-ssh
#   ports:
#   - name: ssh
#     port: 22
#     targetPort: 22
#   type: LoadBalancer  # Or NodePort depending on your environment
---
apiVersion: v1
kind: Service
metadata:
  name: coder-service
spec:
  selector:
    ray.io/node-type: head
    ray.io/cluster: raycluster-autoscaler
  ports:
  - name: code-server
    port: 9999
    targetPort: 9999
  type: LoadBalancer  # Or NodePort depending on your environment
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-code-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /fast_ray
  storageClassName: standard
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-code-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: standard