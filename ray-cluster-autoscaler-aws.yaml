apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: raycluster-autoscaler
spec:
  # The version of Ray you are using. Make sure all Ray containers are running this version of Ray.
  rayVersion: '2.41.0'
  # If `enableInTreeAutoscaling` is true, the Autoscaler sidecar will be added to the Ray head pod.
  # Ray Autoscaler integration is Beta with KubeRay >= 0.3.0 and Ray >= 2.0.0.
  enableInTreeAutoscaling: true
  # `autoscalerOptions` is an OPTIONAL field specifying configuration overrides for the Ray Autoscaler.
  # The example configuration shown below represents the DEFAULT values.
  # (You may delete autoscalerOptions if the defaults are suitable.)
  autoscalerOptions:
    # `upscalingMode` is "Default" or "Aggressive."
    # Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.
    # Default: Upscaling is not rate-limited.
    # Aggressive: An alias for Default; upscaling is not rate-limited.
    upscalingMode: Default
    # `idleTimeoutSeconds` is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
    idleTimeoutSeconds: 60
    # `image` optionally overrides the Autoscaler's container image. The Autoscaler uses the same image as the Ray container by default.
    ## image: ray-ssh:latest
    # `imagePullPolicy` optionally overrides the Autoscaler container's default image pull policy (IfNotPresent).
    imagePullPolicy: Always
    # Optionally specify the Autoscaler container's securityContext.
    securityContext: {}
    env: []
    envFrom: []
    # resources specifies optional resource request and limit overrides for the Autoscaler container.
    # The default Autoscaler resource limits and requests should be sufficient for production use-cases.
    # However, for large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.
    # resources:
    #   limits:
    #     cpu: "1000m"
    #     memory: "1024Mi"
    #   requests:
    #     cpu: "1000m"
    #     memory: "1024Mi"
  # Ray head pod template
  headGroupSpec:
    # The `rayStartParams` are used to configure the `ray start` command.
    # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
    # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
    rayStartParams:
      # Setting "num-cpus: 0" to avoid any Ray actors or tasks being scheduled on the Ray head Pod.
      num-cpus: "0"
      # Use `resources` to optionally specify custom resource annotations for the Ray node.
      # The value of `resources` is a string-integer mapping.
      # Currently, `resources` must be provided in the specific format demonstrated below:
      # resources: '"{\"Custom1\": 1, \"Custom2\": 5}"'
    # Pod template
    template:
      # metadata:
        # labels:
          # app: raycluster-autoscaler
      spec:
        imagePullSecrets:
        - name: ecr-secret
        containers:
        # The Ray head container
        - name: ray-head
          image: 959243851260.dkr.ecr.us-west-2.amazonaws.com/ray-code-robin:v0.0-gpu
          # imagePullPolicy: Never
          imagePullPolicy: Always
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          - containerPort: 22
            name: ssh
          - containerPort: 44217
            name: as-metrics # autoscaler
          - containerPort: 44227
            name: dash-metrics # dashboard
          - containerPort: 9999
            name: code-server
          lifecycle:
            postStart:
              exec:
                command: 
                # command: ["code-server", "--bind-addr", "0.0.0.0:9999", "--auth", "none", "--cert", "false"]
                  - "/bin/bash"
                  - "-c"
                  - |
                    # Redirect all output to log file
                    exec 1> /tmp/poststart.log 2>&1
                    
                    # Print commands as they're executed
                    set -x
                    
                    echo "Starting postStart hook execution at $(date)"
                    
                    echo "Checking environment..."
                    pwd
                    whoami
                    env
                    
                    echo "Starting services..."
                    nohup code-server --bind-addr 0.0.0.0:9999 --auth none --cert false > /tmp/code-server.log 2>&1 &
                    
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "3"
              memory: "4G"
            requests:
              nvidia.com/gpu: 1
              cpu: "3"
              memory: "4G"
          volumeMounts:
            - mountPath: /home/ray/samples
              name: ray-example-configmap
            - mountPath: /tmp/ray
              name: ray-logs
            # - mountPath: /home/ray/workspace  # Mount path inside container
              # name: local-code
            # - name: ssh-authorized-keys
            #   mountPath: /home/ray/.ssh
          env:
            - name: RAY_GRAFANA_IFRAME_HOST
              value: http://127.0.0.1:3000
            - name: RAY_GRAFANA_HOST
              value: http://prometheus-grafana.prometheus-system.svc:80
            - name: RAY_PROMETHEUS_HOST
              value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090
        volumes:
          - name: ray-logs
            emptyDir: {}
          # - name: local-code
          #   persistentVolumeClaim:
          #     claimName: local-code-pvc
          # - name: ssh-authorized-keys
          #   secret:
          #     secretName: ray-ssh-authorized-keys
          #     defaultMode: 0600
          - name: ray-example-configmap
            configMap:
              name: ray-example
              defaultMode: 0777
              items:
                - key: serve_app.py
                  path: serve_app.py
                - key: load_test.py
                  path: load_test.py
  workerGroupSpecs:
  # the Pod replicas in this group typed worker
  - replicas: 0
    minReplicas: 1
    maxReplicas: 10
    # logical group name, for this called small-group, also can be functional
    groupName: small-group
    # If worker pods need to be added, Ray Autoscaler can increment the `replicas`.
    # If worker pods need to be removed, Ray Autoscaler decrements the replicas, and populates the `workersToDelete` list.
    # KubeRay operator will remove Pods from the list until the desired number of replicas is satisfied.
    #scaleStrategy:
    #  workersToDelete:
    #  - raycluster-complete-worker-small-group-bdtwh
    #  - raycluster-complete-worker-small-group-hv457
    #  - raycluster-complete-worker-small-group-k8tj7
    rayStartParams: {}
    # Pod template
    template:
      spec:
        imagePullSecrets:
        - name: ecr-secret
        containers:
        - name: ray-worker
          # image: rayproject/ray:2.41.0
          image: 959243851260.dkr.ecr.us-west-2.amazonaws.com/ray-code-robin:v0.0-gpu
          imagePullPolicy: Always ## added to avoid pulling the image from the registry
          # imagePullPolicy: IfNotPresent ## added to avoid pulling the image from the registry
          volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "3"
              memory: "6G"
            requests:
              nvidia.com/gpu: 1
              cpu: "3"
              memory: "6G"
        volumes:
          - name: ray-logs
            emptyDir: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-example
data:
  serve_app.py: |
    # serve_app.py
    import time
    import random
    import numpy as np
    from typing import Dict

    import ray
    from ray import serve
    from fastapi import FastAPI
    import xgboost as xgb

    app = FastAPI()

    @serve.deployment(
        num_replicas="auto",
        ray_actor_options={"num_cpus": 0.5,
                        #    "num_gpus": 0.5 ## Uncomment to use GPUs
                          },
        autoscaling_config={
            "min_replicas": 1,
            "max_replicas": 20,  # Increase based on your cluster capacity
            "target_num_ongoing_requests_per_replica": 2,  # Lower value = more aggressive scaling
            "upscale_delay_s": 5,  # Reduce delay before scaling up (default is 60s)
            "downscale_delay_s": 60,  # Keep resources longer
            "smoothing_factor": 0.2,  # Lower value = more responsive to traffic spikes
        }
    )
    @serve.ingress(app)
    class XGBoostModel:
        def __init__(self):
            # Create a dummy XGBoost model
            print("Initializing XGBoost model...")
            
            # Create some synthetic data for training
            n_samples = 1000
            n_features = 20
            X = np.random.rand(n_samples, n_features)
            y = np.random.randint(0, 2, n_samples)  # Binary classification
            
            # Create DMatrix
            dtrain = xgb.DMatrix(X, label=y)
            
            # Set XGBoost parameters
            params = {
                'max_depth': 3,
                'eta': 0.1,
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'seed': 42
            }
            
            # Train the model
            print("Training XGBoost model...")
            self.model = xgb.train(params, dtrain, num_boost_round=10)
            print("XGBoost model trained successfully")
            
            # Save feature dimension for inference
            self.n_features = n_features
            
        @app.get("/")
        async def root(self):
            return {"status": "healthy"}
        
        @app.post("/predict")
        async def predict(self, data: Dict):
            # Extract complexity to simulate different workloads
            complexity = data.get("complexity", 1.0)
            complexity = min(max(complexity, 0.1), 5.0)  # Bound between 0.1 and 5.0
            
            # Get features from request or generate random ones
            features = data.get("features", None)
            if features is None or len(features) != self.n_features:
                # Generate random features if not provided correctly
                features = np.random.rand(self.n_features).tolist()
            
            # Start timing
            start_time = time.time()
            
            # Create DMatrix for prediction
            dtest = xgb.DMatrix(np.array([features]))
            
            # Make prediction
            prediction = self.model.predict(dtest)
            
            # Add some artificial delay based on complexity to simulate more processing
            if random.random() < 0.1:  # 10% of requests take longer
                time.sleep(0.5 * complexity)
                
            processing_time = time.time() - start_time
            
            return {
                "prediction": float(prediction[0]),
                "probability": float(prediction[0]),
                "processing_time": processing_time,
                "complexity": complexity,
                "instance_id": ray.get_runtime_context().get_node_id()
            }

    app = XGBoostModel.bind()

  load_test.py: |
    # load_test.py
    import time
    import json
    import random
    import argparse
    import requests
    import threading
    import concurrent.futures
    from typing import Dict
    import numpy as np
    # import matplotlib.pyplot as plt
    from datetime import datetime

    class LoadTester:
        def __init__(self, base_url: str):
            self.base_url = base_url.rstrip('/')
            self.predict_url = f"{self.base_url}/predict"
            self.results = []
            self.instance_counts = {}
            self.lock = threading.Lock()
            
        def make_request(self, complexity: float) -> Dict:
            """Make a single request to the prediction endpoint"""
            try:
                payload = {"complexity": complexity}
                start_time = time.time()
                response = requests.post(self.predict_url, json=payload, timeout=30)
                end_time = time.time()
                
                if response.status_code == 200:
                    result = response.json()
                    result['latency'] = end_time - start_time
                    result['timestamp'] = datetime.now().strftime('%H:%M:%S')
                    
                    with self.lock:
                        self.results.append(result)
                        instance_id = result.get('instance_id', 'unknown')
                        if instance_id in self.instance_counts:
                            self.instance_counts[instance_id] += 1
                        else:
                            self.instance_counts[instance_id] = 1
                    
                    return result
                else:
                    print(f"Error: {response.status_code} - {response.text}")
                    return {"error": response.status_code}
            except Exception as e:
                print(f"Request failed: {str(e)}")
                return {"error": str(e)}
        
        def run_load_test(self, 
                          requests_per_second: int, 
                          test_duration: int,
                          complexity_range: tuple = (0.5, 2.0),
                          ramp_up: bool = True):
            """
            Run a load test with specified RPS
            
            Args:
                requests_per_second: Target RPS
                test_duration: Test duration in seconds
                complexity_range: Range of complexity values
                ramp_up: Whether to gradually ramp up load
            """
            print(f"Starting load test: {requests_per_second} RPS for {test_duration} seconds")
            
            # For tracking actual RPS achieved
            actual_rps_values = []
            timestamp_values = []
            active_instances = []
            instance_counts_over_time = []
            
            # Calculate delay between requests to achieve desired RPS
            delay = 1.0 / requests_per_second if requests_per_second > 0 else 0
            
            # Setup thread pool for concurrent requests
            max_workers = min(50, requests_per_second * 2)  # Limit number of threads
            
            start_time = time.time()
            end_time = start_time + test_duration
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []
                
                # Track stats per second
                last_second = start_time
                requests_this_second = 0
                
                current_time = time.time()
                while current_time < end_time:
                    # Calculate current target RPS if ramping up
                    if ramp_up:
                        elapsed = current_time - start_time
                        progress = min(1.0, elapsed / (test_duration * 0.5))  # Ramp up in first half
                        current_rps = requests_per_second * progress
                    else:
                        current_rps = requests_per_second
                    
                    current_delay = 1.0 / current_rps if current_rps > 0 else 0
                    
                    # Generate random complexity value
                    complexity = random.uniform(complexity_range[0], complexity_range[1])
                    
                    # Submit request
                    future = executor.submit(self.make_request, complexity)
                    futures.append(future)
                    requests_this_second += 1
                    
                    # Calculate stats each second
                    if current_time - last_second >= 1.0:
                        # Record actual RPS
                        actual_rps_values.append(requests_this_second)
                        timestamp_values.append(int(current_time - start_time))
                        
                        # Count active instances
                        with self.lock:
                            instance_counts_over_time.append(len(self.instance_counts))
                        
                        print(f"Time: {int(current_time - start_time)}s, "
                              f"Actual RPS: {requests_this_second}, "
                              f"Active instances: {instance_counts_over_time[-1]}")
                        
                        last_second = current_time
                        requests_this_second = 0
                    
                    # Sleep to maintain RPS
                    time.sleep(current_delay)
                    current_time = time.time()
            
            # Wait for all pending requests to complete
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"Request failed: {str(e)}")
            
            # Print final stats
            total_time = time.time() - start_time
            total_requests = len(self.results)
            actual_rps = total_requests / total_time
            
            print("\n===== Load Test Results =====")
            print(f"Test duration: {total_time:.2f} seconds")
            print(f"Total requests: {total_requests}")
            print(f"Average RPS: {actual_rps:.2f}")
            print(f"Unique instances seen: {len(self.instance_counts)}")
            print("Instance distribution:")
            for instance, count in sorted(self.instance_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  {instance}: {count} requests ({count/total_requests*100:.1f}%)")
            
            # Calculate latency statistics
            latencies = [r['latency'] for r in self.results if 'latency' in r]
            if latencies:
                avg_latency = sum(latencies) / len(latencies)
                p50 = sorted(latencies)[int(len(latencies) * 0.5)]
                p95 = sorted(latencies)[int(len(latencies) * 0.95)]
                p99 = sorted(latencies)[int(len(latencies) * 0.99)]
                
                print("\nLatency Statistics:")
                print(f"  Average: {avg_latency*1000:.2f} ms")
                print(f"  p50: {p50*1000:.2f} ms")
                print(f"  p95: {p95*1000:.2f} ms")
                print(f"  p99: {p99*1000:.2f} ms")
            
            # Generate plots
            # self.plot_results(actual_rps_values, timestamp_values, instance_counts_over_time)
            
            return {
                "total_requests": total_requests,
                "actual_rps": actual_rps,
                "unique_instances": len(self.instance_counts),
                "avg_latency_ms": avg_latency * 1000 if latencies else None
            }
        
        def plot_results(self, actual_rps_values, timestamp_values, instance_counts):
            """Generate plots showing test results"""
            plt.figure(figsize=(12, 10))
            
            # Plot 1: RPS over time
            plt.subplot(2, 1, 1)
            plt.plot(timestamp_values, actual_rps_values, 'b-')
            plt.title('Requests Per Second Over Time')
            plt.xlabel('Time (seconds)')
            plt.ylabel('RPS')
            plt.grid(True)
            
            # Plot 2: Instances over time
            plt.subplot(2, 1, 2)
            plt.plot(timestamp_values, instance_counts, 'r-')
            plt.title('Active Instances Over Time')
            plt.xlabel('Time (seconds)')
            plt.ylabel('Number of Instances')
            plt.grid(True)
            
            plt.tight_layout()
            plt.savefig('load_test_results.png')
            print("Results plotted to load_test_results.png")
            
            # Plot latency distribution
            latencies = [r['latency'] * 1000 for r in self.results if 'latency' in r]  # Convert to ms
            if latencies:
                plt.figure(figsize=(10, 6))
                plt.hist(latencies, bins=50, alpha=0.75)
                plt.title('Latency Distribution')
                plt.xlabel('Latency (ms)')
                plt.ylabel('Count')
                plt.grid(True)
                plt.savefig('latency_distribution.png')
                print("Latency distribution plotted to latency_distribution.png")

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description='Load test Ray Serve deployment')
        parser.add_argument('--url', type=str, default="http://localhost:8000",
                            help='URL of the Ray Serve deployment')
        parser.add_argument('--rps', type=int, default=1,
                            help='Target requests per second')
        parser.add_argument('--duration', type=int, default=60,
                            help='Test duration in seconds')
        parser.add_argument('--min-complexity', type=float, default=0.5,
                            help='Minimum request complexity')
        parser.add_argument('--max-complexity', type=float, default=2.0,
                            help='Maximum request complexity')
        parser.add_argument('--ramp-up', action='store_true',
                            help='Gradually ramp up load')
        
        args = parser.parse_args()
        
        tester = LoadTester(args.url)
        tester.run_load_test(
            requests_per_second=args.rps,
            test_duration=args.duration,
            complexity_range=(args.min_complexity, args.max_complexity),
            ramp_up=args.ramp_up
        )
# ---
# apiVersion: v1
# kind: Service
# metadata:
#   name: ray-head-ssh
# spec:
#   selector:
#     ray.io/node-type: head
#     ray.io/cluster: raycluster-ssh
#   ports:
#   - name: ssh
#     port: 22
#     targetPort: 22
#   type: LoadBalancer  # Or NodePort depending on your environment
---
apiVersion: v1
kind: Service
metadata:
  name: coder-service
spec:
  selector:
    ray.io/node-type: head
    ray.io/cluster: raycluster-autoscaler
  ports:
  - name: code-server
    port: 9999
    targetPort: 9999
  type: LoadBalancer  # Or NodePort depending on your environment
# ---
# apiVersion: v1
# kind: PersistentVolume
# metadata:
#   name: local-code-pv
# spec:
#   capacity:
#     storage: 2Gi
#   accessModes:
#     - ReadWriteOnce
#   hostPath:
#     path: /fast_ray
#   storageClassName: standard
# ---
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: local-code-pvc
# spec:
#   accessModes:
#     - ReadWriteOnce
#   resources:
#     requests:
#       storage: 2Gi
#   storageClassName: standard